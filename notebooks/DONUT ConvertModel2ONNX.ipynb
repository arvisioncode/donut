{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd19d6e9",
   "metadata": {},
   "source": [
    "### Donut conversion to ONNX\n",
    "You need to install transformers to this particular commit,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa3868f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install --upgrade git+https://github.com/huggingface/transformers.git@2e35bac4e73558d334ea5bbf96a1116f7c0d7fb3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d34a823",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip uninstall onnxruntime-training\n",
    "# !pip install onnxruntime-gpu\n",
    "# !pip install transformers[onnx] tokenizers sentencepiece\n",
    "# !pip install optimum[exporters]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d715d3",
   "metadata": {},
   "source": [
    "Check if the code has been added to an stable version, it seems that commit has been merged already,\n",
    "\n",
    "https://github.com/huggingface/transformers/pull/19254\n",
    "\n",
    "I tried different tolerances, I remember we didn't need to go as large as 1e-2, but something in between 1e-2 and 1e-3, I tried all these(don't remember exactly which one was the successful one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b7289e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Framework not requested. Using torch to export to ONNX.\n",
      "C:\\Users\\MSI\\anaconda3\\lib\\site-packages\\torch\\functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ..\\aten\\src\\ATen\\native\\TensorShape.cpp:2157.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "Unsupported PyTorch version for this model. Minimum required is 1.11, got: 1.10.1+cu113\n",
      "Using framework PyTorch: 1.10.1+cu113\n",
      "C:\\Users\\MSI\\anaconda3\\lib\\site-packages\\torch\\onnx\\utils.py:90: UserWarning: 'enable_onnx_checker' is deprecated and ignored. It will be removed in the next PyTorch release. To proceed despite ONNX checker failures, catch torch.onnx.ONNXCheckerError.\n",
      "  warnings.warn(\"'enable_onnx_checker' is deprecated and ignored. It will be removed in \"\n",
      "C:\\Users\\MSI\\anaconda3\\lib\\site-packages\\torch\\onnx\\utils.py:103: UserWarning: `use_external_data_format' is deprecated and ignored. Will be removed in next PyTorch release. The code will work as it is False if models are not larger than 2GB, Otherwise set to False because of size limits imposed by Protocol Buffers.\n",
      "  warnings.warn(\"`use_external_data_format' is deprecated and ignored. Will be removed in next \"\n",
      "C:\\Users\\MSI\\anaconda3\\lib\\site-packages\\transformers\\models\\donut\\modeling_donut_swin.py:230: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if num_channels != self.num_channels:\n",
      "C:\\Users\\MSI\\anaconda3\\lib\\site-packages\\transformers\\models\\donut\\modeling_donut_swin.py:220: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if width % self.patch_size[1] != 0:\n",
      "C:\\Users\\MSI\\anaconda3\\lib\\site-packages\\transformers\\models\\donut\\modeling_donut_swin.py:223: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if height % self.patch_size[0] != 0:\n",
      "C:\\Users\\MSI\\anaconda3\\lib\\site-packages\\transformers\\models\\donut\\modeling_donut_swin.py:536: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if min(input_resolution) <= self.window_size:\n",
      "C:\\Users\\MSI\\anaconda3\\lib\\site-packages\\transformers\\models\\donut\\modeling_donut_swin.py:136: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  batch_size, height // window_size, window_size, width // window_size, window_size, num_channels\n",
      "C:\\Users\\MSI\\anaconda3\\lib\\site-packages\\transformers\\models\\donut\\modeling_donut_swin.py:148: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  windows = windows.view(-1, height // window_size, width // window_size, window_size, window_size, num_channels)\n",
      "C:\\Users\\MSI\\anaconda3\\lib\\site-packages\\transformers\\models\\donut\\modeling_donut_swin.py:622: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  was_padded = pad_values[3] > 0 or pad_values[5] > 0\n",
      "C:\\Users\\MSI\\anaconda3\\lib\\site-packages\\transformers\\models\\donut\\modeling_donut_swin.py:623: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if was_padded:\n",
      "C:\\Users\\MSI\\anaconda3\\lib\\site-packages\\transformers\\models\\donut\\modeling_donut_swin.py:411: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  batch_size // mask_shape, mask_shape, self.num_attention_heads, dim, dim\n",
      "C:\\Users\\MSI\\anaconda3\\lib\\site-packages\\transformers\\models\\donut\\modeling_donut_swin.py:682: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  height_downsampled, width_downsampled = (height + 1) // 2, (width + 1) // 2\n",
      "C:\\Users\\MSI\\anaconda3\\lib\\site-packages\\transformers\\models\\donut\\modeling_donut_swin.py:266: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  should_pad = (height % 2 == 1) or (width % 2 == 1)\n",
      "C:\\Users\\MSI\\anaconda3\\lib\\site-packages\\transformers\\models\\donut\\modeling_donut_swin.py:267: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if should_pad:\n",
      "Validating ONNX model...\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\MSI\\anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\MSI\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\MSI\\anaconda3\\lib\\site-packages\\transformers\\onnx\\__main__.py\", line 180, in <module>\n",
      "    main()\n",
      "  File \"C:\\Users\\MSI\\anaconda3\\lib\\site-packages\\transformers\\onnx\\__main__.py\", line 107, in main\n",
      "    validate_model_outputs(\n",
      "  File \"C:\\Users\\MSI\\anaconda3\\lib\\site-packages\\transformers\\onnx\\convert.py\", line 392, in validate_model_outputs\n",
      "    ref_outputs = reference_model(**reference_model_inputs)\n",
      "  File \"C:\\Users\\MSI\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1102, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"C:\\Users\\MSI\\anaconda3\\lib\\site-packages\\transformers\\models\\donut\\modeling_donut_swin.py\", line 914, in forward\n",
      "    encoder_outputs = self.encoder(\n",
      "  File \"C:\\Users\\MSI\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1102, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"C:\\Users\\MSI\\anaconda3\\lib\\site-packages\\transformers\\models\\donut\\modeling_donut_swin.py\", line 756, in forward\n",
      "    layer_outputs = layer_module(hidden_states, input_dimensions, layer_head_mask, output_attentions)\n",
      "  File \"C:\\Users\\MSI\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1102, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"C:\\Users\\MSI\\anaconda3\\lib\\site-packages\\transformers\\models\\donut\\modeling_donut_swin.py\", line 677, in forward\n",
      "    layer_outputs = layer_module(hidden_states, input_dimensions, layer_head_mask, output_attentions)\n",
      "  File \"C:\\Users\\MSI\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1102, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"C:\\Users\\MSI\\anaconda3\\lib\\site-packages\\transformers\\models\\donut\\modeling_donut_swin.py\", line 631, in forward\n",
      "    layer_output = self.intermediate(layer_output)\n",
      "  File \"C:\\Users\\MSI\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1102, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"C:\\Users\\MSI\\anaconda3\\lib\\site-packages\\transformers\\models\\donut\\modeling_donut_swin.py\", line 501, in forward\n",
      "    hidden_states = self.dense(hidden_states)\n",
      "  File \"C:\\Users\\MSI\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1102, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"C:\\Users\\MSI\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\linear.py\", line 103, in forward\n",
      "    return F.linear(input, self.weight, self.bias)\n",
      "  File \"C:\\Users\\MSI\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\", line 1848, in linear\n",
      "    return torch._C._nn.linear(input, weight, bias)\n",
      "RuntimeError: [enforce fail at ..\\c10\\core\\CPUAllocator.cpp:76] data. DefaultCPUAllocator: not enough memory: you tried to allocate 943718400 bytes.\n"
     ]
    }
   ],
   "source": [
    "## DONUT: DOCVQA\n",
    "model_ckpt = \"naver-clova-ix/donut-base-finetuned-docvqa\"\n",
    "!python -m transformers.onnx --model={model_ckpt} --feature=vision2seq-lm onnx/docvqa --atol 7e-3\n",
    "# if there is an error try modifying atol in [1e-3, 7e-3]\n",
    "\n",
    "## DONUT: CLASSIFIER RVLCDIP\n",
    "model_ckpt = \"naver-clova-ix/donut-base-finetuned-rvlcdip\"\n",
    "!python -m transformers.onnx --model={model_ckpt} --feature=vision2seq-lm onnx/rvlcdip --atol 7e-3\n",
    "# if there is an error try modifying atol in [1e-3, 7e-3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043d9f13",
   "metadata": {},
   "source": [
    "### Quantizing\n",
    "#### decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c2ef94",
   "metadata": {},
   "outputs": [],
   "source": [
    "!olive optimize --model_path decoder_model.onnx --input_names input_ids,attention_mask,encoder_hidden_states --input_shapes [[-1,-1],[-1,-1],[-1,-1,1024]] --quantization_enabled --dynamic_batching_size 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dda5799",
   "metadata": {},
   "source": [
    "#### encoder/decoder with optimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9cc4cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimum.onnxruntime import AutoQuantizationConfig\n",
    "from onnxruntime.quantization.qdq_quantizer import QDQQuantizer\n",
    "from onnxruntime.quantization.onnx_quantizer import ONNXQuantizer\n",
    "from optimum.onnxruntime import ORTQuantizableOperator\n",
    "import onnx\n",
    "\n",
    "quantization_config = AutoQuantizationConfig.avx512_vnni(is_static=False, per_channel=False)\n",
    "quantizer_factory = QDQQuantizer if False else ONNXQuantizer\n",
    "\n",
    "# here pick either model\n",
    "path = \"./decoder.onnx\"\n",
    "onnx_model = onnx.load(path)\n",
    "quantizer = quantizer_factory(\n",
    "                model=onnx_model,\n",
    "                static=quantization_config.is_static,\n",
    "                per_channel=quantization_config.per_channel,\n",
    "                mode=quantization_config.mode,\n",
    "                weight_qType=quantization_config.weights_dtype,\n",
    "                activation_qType=quantization_config.activations_dtype,\n",
    "                tensors_range=None,\n",
    "                reduce_range=quantization_config.reduce_range,\n",
    "                nodes_to_quantize=quantization_config.nodes_to_quantize,\n",
    "                nodes_to_exclude=quantization_config.nodes_to_exclude,\n",
    "                op_types_to_quantize=[\n",
    "                    operator.value if isinstance(operator, ORTQuantizableOperator) else operator\n",
    "                    for operator in quantization_config.operators_to_quantize\n",
    "                ],\n",
    "                extra_options={\n",
    "                    \"WeightSymmetric\": quantization_config.weights_symmetric,\n",
    "                    \"ActivationSymmetric\": quantization_config.activations_symmetric,\n",
    "                    \"EnableSubgraph\": False,\n",
    "                    \"ForceSymmetric\": quantization_config.activations_symmetric\n",
    "                    and quantization_config.weights_symmetric,\n",
    "                    \"AddQDQPairToWeight\": quantization_config.qdq_add_pair_to_weight,\n",
    "                    \"DedicatedQDQPair\": quantization_config.qdq_dedicated_pair,\n",
    "                    \"QDQOpTypePerChannelSupportToAxis\": quantization_config.qdq_op_type_per_channel_support_to_axis,\n",
    "                },\n",
    "            )\n",
    "\n",
    "quantizer.quantize_model()\n",
    "quantizer.model.save_model_to_file(\"quantized_by_optimum.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ddcf03c",
   "metadata": {},
   "source": [
    "### Handling the memory issues\n",
    "You should go to this file,\n",
    "\n",
    "/home/ubuntu/.local/lib/python3.7/site-packages/transformers/onnx/convert.py\n",
    "\n",
    "in method `validate_model_outputs()`\n",
    "\n",
    "and patch things similar to the changes here.\n",
    "This is just reducing the batch size from 3 to 1. I we really want to try 3 different examples, we could just run the validation 3 times, each time with batch size 1, so we don't kill the memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db6bca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # generate inputs with a different batch_size and seq_len that was used for conversion to properly test\n",
    "    # dynamic input shapes.\n",
    "    if is_torch_available() and issubclass(type(reference_model), PreTrainedModel):\n",
    "        reference_model_inputs = config.generate_dummy_inputs(\n",
    "            preprocessor,\n",
    "            batch_size=1, #config.default_fixed_batch + 1,\n",
    "            seq_length=config.default_fixed_sequence + 1,\n",
    "            framework=TensorType.PYTORCH,\n",
    "        )\n",
    "    else:\n",
    "        reference_model_inputs = config.generate_dummy_inputs(\n",
    "            preprocessor,\n",
    "            batch_size=1, #config.default_fixed_batch + 1,\n",
    "            seq_length=config.default_fixed_sequence + 1,\n",
    "            framework=TensorType.TENSORFLOW,\n",
    "        )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9 (tags/v3.7.9:13c94747c7, Aug 17 2020, 18:58:18) [MSC v.1900 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "83837d4b0c7e3696993002b18347dd25aabfa174e790b25d515ceedf3fe9df40"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
